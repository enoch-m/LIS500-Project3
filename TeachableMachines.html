<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="styles.css">
    <title>Teachable Machines</title>
</head>

<body>
    <header>
        <!-- Website heading -->
        <h1>Teachable Machines - Project Statements</h1>

        <!-- Navigation for webpage -->
        <a href="index.html">Home</a> | <a href="AboutUs.html">About Us</a>
        | <a href="Resources.html">Resources</a> | <a href="TechHeroes.html">Tech Heroes</a> | Teachable Machines
    </header>

    <span class="TeachableMachines">
        <div class="project-statements">
            <!-- Project Statements -->
            <h1>Project Overview</h1>
            <p>
                Our project takes the lessons learned from the book “Unmasking AI” by Joy Buolamwini and applies it to
                teaching a machine. We use an online tool called Teachable Machine 2.0, that trains a computer to
                recognize
                images, sounds, poses, etc. We plan to explore these ideas by creating a machine learning model using
                Teachable Machine 2.0 to train a model to recognize everyday objects, such as an iPhone, a pair of
                scissors,
                a computer mouse, and a pair of headphones. Through this hands-on approach, we aim to better understand
                how
                machine learning models are trained and the importance of diverse, balanced datasets in achieving
                accurate
                predictions.
            </p>
            <h2>Lessons learned from ‘Unmasking AI’</h2>
            <p>
            <h3>Oppressive Reality in AI</h3>
            From politics to social structures to even AI, there are many examples of social injustices. We learn how AI
            is
            trained on data, but where the data comes from derives from humans. As a result there is a bias found within
            AI
            systems. Buolamwini highlights this in her book “Unmasking AI” in numerous cases such as her project with
            the
            aspire mirror and public surveillance. We see how the use of AI causes misinformation and false results,
            often
            leaving minorities with the short end of the stick. Buolamwini’s project wasn’t able to recognize her face
            due
            to her skin tone, but when she put a white mask on, the mirror was able to recognize her. Public
            surveillance
            accused an innocent person based on their skin tone. The reality is that AI is based on data derived from
            human
            biases.
            </p>
            <p>
                For us, this reality of AI is a wake up call. On one hand, we know that there are biases in the world
                that
                we live in, we see it all the time. But on the other hand, these realizations shock us as AI becomes
                more
                relevant in our lives. We have tools like ChatGPT, CoPilot, and AI embedded in the apps we use. Even
                this
                project shows us how accessible training artificial intelligence is. Knowing this gives us a sense of
                caution and awareness of the responsibility that AI development carries.
            </p>

            <p>
            <h3>Effects of Algorithmic Biases on Society</h3>
            Buolamwini went to great lengths to fight against those who use AI for personal gain at the expense of
            marginalized groups. She showed examples of her fighting against these injustices within the Brooklyn
            tenants
            when AI systems were installed, leading to gentrification and displacement as well as her fight in court to
            advocate for justice so that big tech companies can’t use these biases in their favor.
            </p>
            <p>
                As the readers, we can see the impact that these algorithmic biases have on real people. Buolamwini was
                able
                to show us with the testimonies of people who are being affected negatively by this. We see that the
                Brooklyn tenants have their rent increased and companies that take advantage of them through new AI
                tools
                implemented into the building. Imagine all the stories that are not told. How many people have already
                been
                victims to algorithmic bias and didn’t have support. It shows the scope of power that AI has. And it is
                usually those in power who have control over it.
            </p>

            <p>
            <h3>Accountability for those who use AI</h3>
            When Buolamwini went to court to encourage lawmakers to create better ethical structures for AI, she
            described
            the emotional weight to the experience. We can understand that there is a weight to this when we understand
            that
            algorithmic bias is rooted in injustices and racism embedded in American history. From slavery to
            segregation to
            Jim Crow laws to injustices in AI, the concept is the same. It is all rooted in the same problem. It might
            look
            more subtle through AI, but this shows us the reality of bias in AI. The users of AI have a lot of power.
            Buolamwini also highlights the role that big tech companies have. She talks about how they often address
            injustices via tokenism. Tokenism is when these big tech companies mention, and include, minorities for the
            sole
            purpose of saying they treat minorities well and fairly. But in reality, these efforts don’t address the
            roots
            of these issues. In fact, Buolamwini mentions how these efforts undermine genuine efforts to create a more
            equitable environment.
            </p>

            <p>
            <h3>What can we do in response to Algorithmic Bias?</h3>
            So in response to that, we want to take Buolamwini’s efforts and apply them into how we treat AI. She
            emphasizes
            that AI is a tool that is here to stay. It isn’t about avoiding the AI, but rather learning how to embrace
            it.
            We don’t think that AI is inherently bad, but what makes it biased is the data that is fed into it. Towards
            the
            end of the book, Buolamwini describes AI as a sword of knowledge. This sword can be both a weapon and a tool
            for
            liberation. There is a responsibility that comes with this knowledge. Since we use AI in our daily lives, we
            have a role to play in this. How we treat and use AI affects people on a larger scale.
            </p>
            <p>
            <h2>Motivations and Objectives for the Project</h2>
            <p>AI systems are becoming increasingly integrated into our daily lives, from personal assistants to
                advanced
                decision-making algorithms. While these technologies have immense potential, they also carry significant
                risks when biases in training data are overlooked. Buolamwini’s work serves as a wake-up call, revealing
                how
                biases in AI systems can perpetuate social injustices.</p>
            <p>So with this project, we want to find ways where we can use AI to learn. A benefit to a seemingly
                limitless
                catalog of knowledge is that there is so much to explore. In order to learn, we have to be curious and
                have
                humility. When we know that we don’t have all the answers, we can see how much more we can learn, and we
                can
                use AI as a means to get there. We think that this requires a perspective shift. We can’t be overly
                reliant
                on AI to solve things for us and we have to make sure that our goals are not affecting people
                negatively.
            </p>
            <p>Our primary objective is to create a simple yet effective machine learning model and use it as a
                framework to
                understand the mechanics of machine learning, investigate how biases in data affect model outcomes, and
                reflect on the ethical responsibilities associated with AI development.</p>
            </p>

        </div>

        <div class="embedded-video">
            <!-- Embedded Video of our algorithm -->
            <h1>Video of our algorithm</h1>
            <video width="550" height="310" controls>
                <source src="Images/model_training_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <!-- A link for others to try our model -->
            <h2><a href="https://teachablemachine.withgoogle.com/models/n7gmdMGPB/">Try the Model!</a></h2>
        </div>

        <div class="ML-application-experiments">
            <!-- Content can be split into different pages-->
            <h1>Project Scope</h1>
            <p>Classification: Personal Commen Accessories</p>
            <p>Range: Four different accessories</p>
            <p>Item List: iPhone, Scissor, Mouse, Headphone</p>
            <p>Image Samples: around 123 to 223 for each classified item</p>

            <h1>Process</h1>
            <ol>
                <li>
                    <p>Gather image samples using Webcam from each angle of the item</p><img src="Images/gather_img_sample.png"
                        alt="gather image sample" width=".25">
                </li>
                <li>
                    <p>Gather image samples for more classes</p><img src="Images/more_sample_classes.png"
                        alt="gather more samples" width=".25">
                </li>
                <li>
                    <p>Train the model with the captured samples from different classes</p><img
                        src="Images/training_model.png" alt="train the model" width=".25">
                </li>
                <li>
                    <p>Training for samples complete</p><img src="Images/train_complete.png" alt="training complete"
                        width=".25">
                </li>
                <li>
                    <p>Exported the model with a sharable link for use</p><img src="Images/export_model.png"
                        alt="export the model with url" width=".25">
                </li>
            </ol>
            <p>A number of image samples were captured using Teachable Machine's Webcam, including a set of samples for
                'Neutral,' which represents when there's nothing in the frame but only the background.</p>
            <p>While capturing image samples using the Webcam, the item occasionally moves out of the camera's view,
                leaving
                mostly the background captured. These background samples need to be manually deleted to reduce bias.</p>
            <p>Perhaps because the sample used for the mouse is white, testing with a black mouse occasionally results
                in it
                being misclassified as an iPhone, which suggest we might need more varied samples.</p>
            <p>Overall, the machine learning model is able to successfully identify all the accessories.</p>
        </div>

        <div class="reflections">
            <!-- Our thoughts on the project -->
            <h1>Lessons Learned</h1>
            <p>
                This project allowed us to implement many ideas that we’ve learned in class into practice. We’ve learned 
		not only the design styles and practices from class but the nature inclusion and AI in our world. Through 
		the inequalities that we have seen in the real world or online, we realize that we have our part to play. 
		This project showed us how accessible training models can be and how easily data can be fed. So it gives 
		us a sense of responsibility to do our part in using AI in the right ways.
            </p>
	    <p>
		In relation to the biases and imperfections that Buolamwini depicts, we have seen first hand the false 
		positives that AI brings. One of our biggest issues in our training model is depicting objects based on 
		their color. We used objects such as a white mouse and black iPhone to train our model, and as a result, 
		the model depicted anything black to be an iPhone and white objects became inconclusive. These results 
		remind us of the aspire mirror project, where the white mask was detected but Buolamwini’s face was not. 
		The issue was not within the machine itself, but with what data was being fed into it. The issue, therefore, 
		lies not with the machine itself but with the limitations in the data we chose to provide it.
	    </p>
            <h2>Awareness </h2>
            One key takeaway is the importance of awareness. Like Buolamwini’s examples of bias in public surveillance
            or
            facial recognition,
            we realized how easily injustice can be perpetuated if we ignore underlying biases in AI. These lessons
            aren’t
            just theoretical but also
            practical, especially as we see the growing impact of AI in our daily lives through tools like ChatGPT and
            other
            models.
            This project emphasized to us that understanding bias is essential for ethical AI use. It is important to
            understand how to keep ethical practices
            ourselves and explain to others their importance.
            </p>

			<p>
			Perhaps the most important lesson from Unmasking AI is that fairness in AI is not a 
			one-time goal but an ongoing process. Buolamwini emphasizes that addressing bias requires continuous 
			evaluation, iteration, and engagement with diverse perspectives. This means not only refining datasets 
			and models but also considering the broader societal impacts of AI technologies. In our project, this 
			commitment manifested as a willingness to iterate and improve, even as we faced limitations in resources 
			and scope. 
</p>
			</p>

            <p>
                Moreover, we reflected on our role as future developers and users of AI. Just as Buolamwini stressed the
                need for accountability in AI development,
                we recognized that we must actively question the data sources, training processes, and real-world
                applications of our models. This is
                for both our own models and those we interact with in our education and professional lives. Ethical AI
                can
                be achieved, and it is up to learn
                and implement these practices.
            </p>
            <p>During our testing, we encountered a real-world example of how training data affects model outcomes.
                Perhaps
                because the sample images used for the mouse were all white, testing the model with a black mouse
                occasionally resulted in it being misclassified as an iPhone. This finding highlights the importance of
                using diverse and varied samples when training models to ensure they can generalize well. In our case,
                this
                misclassification wasn’t critical, but in a real-world application, similar oversights could lead to
                significant errors or reinforce harmful biases. This further reinforced the need for vigilance and
                thoroughness in curating datasets for machine learning. By addressing these issues during the training
                process, we can take a step toward building fairer and more reliable AI systems.</p>

        </div>
    </span>

    <footer>
        <h2>Let's Keep in Touch!</h2>

        <div>
            <a href="mailto:xlu297@wisc.edu">xlu297@wisc.edu</a>
            <a href="mailto:etmoon@wisc.edu">etmoon@wisc.edu</a>
            <a href="mailto:cgfaherty@wisc.edu">cgfaherty@wisc.edu</a>
        </div>

        <p>Copyright 2024 by Alisa Lu, Enoch Moon, Cal Faherty</p>

    </footer>
</body>

</html>
