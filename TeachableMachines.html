<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="stylepage.css">
    <title>Project 3 - Teachable  Machines</title>
</head>

<body>
    <header>
        <!-- Website heading -->
        <h1>Teachable Machines - Project Statements</h1>

        <!-- Navigation for webpage -->
        Home | <a href="AboutUs.html">About Us</a>
        | <a href="Resources.html">Resources</a> | <a href="TechHeroes.html">Tech Heroes</a>
    </header>

    <div class="project-statements">
        <!-- Project Statements -->
        <h1>Project Overview</h1> 
            <p>
                Our project takes the lessons learned from the book “Unmasking AI” by Joy Buolamwini and applies it to teaching a machine. We use an online tool called Teachable Machine 2.0, that trains a computer to recognize images, sounds, poses, etc.
            </p>
        <h2>Lessons learned from ‘Unmasking AI’</h2>
            <p>
                <h3>Oppressive Reality in AI</h3>
                From politics to social structures to even AI, there are many examples of social injustices. We learn how AI is trained on data, but where the data comes from derives from humans. As a result there is a bias found within AI systems. Buolamwini highlights this in her book “Unmasking AI” in numerous cases such as her project with the aspire mirror and public surveillance. We see how the use of AI causes misinformation and false results, often leaving minorities with the short end of the stick. Buolamwini’s project wasn’t able to recognize her face due to her skin tone, but when she put a white mask on, the mirror was able to recognize her. Public surveillance accused an innocent person based on their skin tone. The reality is that AI is based on data derived from human biases.
            </p>
            <p>
                For us, this reality of AI is a wake up call. On one hand, we know that there are biases in the world that we live in, we see it all the time. But on the other hand, these realizations shock us as AI becomes more relevant in our lives. We have tools like ChatGBT, CoPilot, and AI embedded in the apps we use. Even this project shows us how accessible training artificial intelligence is. Knowing this gives us a sense of caution and awareness of the responsibility that AI development carries.
            </p>

            <p>
                <h3>Effects of Algorithmic Biases on Society</h3>
                Buolamwini went to great lengths to fight against those who use AI for personal gain at the expense of marginalized groups. She showed examples of her fighting against these injustices within the Brooklyn tenants when AI systems were installed, leading to gentrification and displacement as well as her fight in court to advocate for justice so that big tech companies can’t use these biases in their favor.
            </p>
            <p>
                As the readers, we can see the impact that these algorithmic biases have on real people. Buolamwini was able to show us with the testimonies of people who are being affected negatively by this. We see that the Brooklyn tenants have their rent increased and companies that take advantage of them through new AI tools implemented into the building. Imagine all the stories that are not told. How many people have already been victims to algorithmic bias and didn’t have support. It shows the scope of power that AI has. And it is usually those in power who have control over it.
            </p>

            <p>
                <h3>Accountability for those who use AI</h3>
                When Buolamwini went to court to encourage lawmakers to create better ethical structures for AI, she described the emotional weight to the experience. We can understand that there is a weight to this when we understand that algorithmic bias is rooted in injustices and racism embedded in American history. From slavery to segregation to Jim Crow laws to injustices in AI, the concept is the same. It is all rooted in the same problem. It might look more subtle through AI, but this shows us the reality of bias in AI. The users of AI have a lot of power. Buolamwini also highlights the role that big tech companies have. She talks about how they often address injustices via tokenism. Tokenism is when these big tech companies mention, and include, minorities for the sole purpose of saying they treat minorities well and fairly. But in reality, these efforts don’t address the roots of these issues. In fact, Buolamwini mentions how these efforts undermine genuine efforts to create a more equitable environment.
            </p>

            <p>
                <h3>What can we do in response to Algorithmic Bias?</h3>
                So in response to that, we want to take Buolamwini’s efforts and apply them into how we treat AI. She emphasizes that AI is a tool that is here to stay. It isn’t about avoiding the AI, but rather learning how to embrace it. We don’t think that AI is inherently bad, but what makes it biased is the data that is fed into it. Towards the end of the book, Buolamwini describes AI as a sword of knowledge. This sword can be both a weapon and a tool for liberation. There is a responsibility that comes with this knowledge. Since we use AI in our daily lives, we have a role to play in this. How we treat and use AI affects people on a larger scale.
            </p>
            <p>
                So with this project, we want to find ways where we can use AI to learn. A benefit to a seemingly limitless catalog of knowledge is that there is so much to explore. In order to learn, we have to be curious and have humility. When we know that we don’t have all the answers, we can see how much more we can learn, and we can use AI as a means to get there. We think that this requires a perspective shift. We can’t be overly reliant on AI to solve things for us and we have to make sure that our goals are not affecting people negatively.
            </p>

    </div>

    <div class="Embedded Video">
        <!-- Embedded Video of our algorithm -->
    </div>

    <div class="ML application/experiments">
        <!-- Content can be split into different pages-->
    </div>

    <div class="Reflections">
        <!-- Our thoughts on the project -->
    </div>

    <footer>
        <h2>Let's Keep in Touch!</h2>

        <div>
            <a href="mailto:xlu297@wisc.edu">xlu297@wisc.edu</a>
            <a href="mailto:etmoon@wisc.edu">etmoon@wisc.edu</a>
        </div>

        <p>Copyright 2024 by Alisa Lu, Enoch Moon, Cal</p>

    </footer>
</body>

</html>